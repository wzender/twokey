# Add `phoneme_key` per sentence (industrial-grade Voice Mode)

## What youâ€™re adding
For every sentence record, add a new field:

- `phoneme_key`: a **consonant skeleton** (plus a few stable markers) that is robust to:
  - Hebrew-accent substitutions (Ù‚â†’Ùƒ, Ø¹ dropped, emphatics softened)
  - ASR spelling variability
  - Optional Ø§Ù„Ù€ / Ø¹Ø§Ù„Ù€ surface forms
  - Small word-order noise

Youâ€™ll use `phoneme_key` **only for evaluation**, never for display.

---

## Updated JSON schema (per sentence)
```json
{
  "lesson_id": 2,
  "sentence_id": 3,
  "prompt_he": "×”×•× ×™×©×‘ ×¢×œ ×”×›×™×¡× ×•××›×œ ×¤×œ××¤×œ",
  "accepted_answers": "Ù‡Ùˆ Ù‚Ø¹Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ±Ø³ÙŠ ÙˆØ£ÙƒÙ„ ÙÙ„Ø§ÙÙ„",
  "answer_he_tatiq": "×”×•Ö¼Ù‘ ×§Ö·×¢Ö·×“ ×¢Ö·×œÖ·× ×Öµ×œÖ°×›Ö¼Ö»×¨Ö°×¡Ö´×™ ×•Ö¼×Ö·×›Ö¼Ö·×œ ×¤Ö·×œÖ·××¤Öµ×œ",
  "phoneme_key": "HW|Q3D|3L|KRSY|W|AKL|FLFL"
}
```

Notes:
- The delimiter format is intentionally simple: `|` between tokens.
- Example above shows **conceptual** output; your generator defines the exact mapping rules below.

---

## Generator: how to compute `phoneme_key`

### Input preference order
1) If `accepted_answers` is available (Arabic script) â†’ use it (best)
2) Else fall back to `answer_he_tatiq` (Hebrew transliteration) â†’ use translit rules

### Step 1 â€” Normalize text
Apply to input text before tokenizing:
- Remove punctuation, tatweel, diacritics
- Normalize Alef: Ø£/Ø¥/Ø¢ â†’ Ø§
- Normalize Ya: Ù‰ â†’ ÙŠ
- Remove definite article effect for keying:
  - Strip leading `Ø§Ù„` from nouns (comparison-only)
- Expand/normalize clitics:
  - `Ø¹Ø§Ù„` â†’ `Ø¹Ù„Ù‰ Ø§Ù„` (comparison-only)
- Collapse whitespace

### Step 2 â€” Tokenize
Split on whitespace into tokens.

### Step 3 â€” Drop weak tokens (optional but recommended)
Remove tokens that ASR frequently mangles and donâ€™t carry meaning:
- Coordinators and fillers if desired: Ùˆ, ÙŠØ§ (keep `Ùˆ` only if you want sequencing stability)
- If you keep `Ùˆ`, keep it as token `W`.

### Step 4 â€” Map each token to a consonant key
For each token, create a canonical consonant skeleton:

#### 4.1 Consonant extraction (Arabic script)
- Keep only letters from: Ø¨ Øª Ø« Ø¬ Ø­ Ø® Ø¯ Ø° Ø± Ø² Ø³ Ø´ Øµ Ø¶ Ø· Ø¸ Ø¹ Øº Ù Ù‚ Ùƒ Ù„ Ù… Ù† Ù‡ Ùˆ ÙŠ Ø¡
- Drop short vowels (diacritics already removed)
- Optionally keep long-vowel carriers only if they are consonantal in context:
  - Keep Ùˆ / ÙŠ when they are part of the stem (practical rule: always keep them; itâ€™s fine)

#### 4.2 Phoneme tolerance mapping (collapse confusables)
Apply these canonicalizations **inside the key**:
- Ù‚ â†’ K
- Ùƒ â†’ K
- Ø¹ â†’ A (or drop entirely; choose one and stick to it)
- Ø¡ â†’ (drop)
- Ø­ â†’ H
- Ù‡ â†’ H
- Ø® â†’ KH (or H if you want heavier tolerance)
- Ø« â†’ T
- Ø° â†’ D
- Ø¸ â†’ Z
- Øµ â†’ S
- Ø¶ â†’ D
- Ø· â†’ T
- Øº â†’ GH (or R if you want heavy Hebrew tolerance)
- Ø´ â†’ SH (or S if you want heavier tolerance)

Everything else maps to its Latin-ish mnemonic:
- Ø¨ B, Øª T, Ø¬ J, Ø¯ D, Ø± R, Ø² Z, Ø³ S, Ù F, Ù„ L, Ù… M, Ù† N, Ùˆ W, ÙŠ Y

### Step 5 â€” Token compaction
To keep keys stable and short:
- Remove repeated identical letters: `KKRRSY` â†’ `KRSY`
- Keep at least 2 characters per token unless it is a pronoun/particle you intentionally keep

### Step 6 â€” Preserve meaning-critical anchors (recommended)
Always preserve separate tokens for:
- Pronouns: Ù‡Ùˆ/Ù‡ÙŠ/Ø¥Ù†Øª/Ø¥Ø­Ù†Ø§/Ù‡Ù…â€¦
- Prepositions: ÙÙŠ/Ø¹Ù„Ù‰/Ù…Ø¹/Ù„Ù€
- Main verbs: ÙØªØ­/Ù‚Ø¹Ø¯/Ø£ÙƒÙ„/Ø³Ø£Ù„â€¦

This prevents the key from accepting â€œwrong meaning but similar sound.â€

### Step 7 â€” Join into `phoneme_key`
Join mapped tokens with `|`.

---

## Using `phoneme_key` in EVALUATE

### Evaluation pipeline (replace raw-string compare)
1) Compute `student_key` from student ASR transcript using the **same generator**
2) Compare `student_key` to `expected phoneme_key` with:
   - Sliding token window (Â±2 tokens)
   - Must-pass anchors (pronouns, verb, prepositions)
   - Overall token match threshold (e.g., â‰¥70%)

### Pass rules
CORRECT if:
- All anchors match
- Token match â‰¥ threshold

INCORRECT if:
- Any anchor fails
- Token match below threshold

### Coaching
If anchors pass but a mapped phoneme indicates accent drift (e.g., KHâ†’H), accept but coach briefly in Hebrew.

---

## Practical threshold recommendations
- Short sentences (â‰¤5 tokens): require â‰¥80% token match
- Medium (6â€“9 tokens): â‰¥70%
- Long (10+ tokens): â‰¥65%

Always enforce anchors strictly regardless of length.

---

## Implementation note (no fluff)
If you ever change the mapping rules, you must **recompute phoneme_key for all sentences**,
or youâ€™ll get silent false negatives. The machine will not forgive you. ğŸ™‚

---

### END OF FILE
